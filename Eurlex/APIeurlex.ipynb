{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:190: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:190: SyntaxWarning: invalid escape sequence '\\d'\n",
      "/tmp/ipykernel_15077/4053917348.py:190: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  text_name = text_name.split(re.findall('\\d+',text_name)[0],1)[1] if '\\u25ba' in text_name else text_name\n",
      "/opt/conda/lib/python3.12/site-packages/urllib3/connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'eur-lex.europa.eu'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.12/site-packages/urllib3/connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'eur-lex.europa.eu'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.12/site-packages/urllib3/connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'eur-lex.europa.eu'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import time\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "import ssl\n",
    "import xml.etree.ElementTree as ET\n",
    "from glob import glob\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "from bs4 import BeautifulSoup\n",
    "from requests.utils import quote\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "LOGIN_EURLEX = os.getenv(\"client_id_legifrance\")\n",
    "MOT_DE_PASSE_EURLEX = os.getenv(\"client_secret_legifrance\")\n",
    "\n",
    "def get_value_if_cellar(root):\n",
    "    \"\"\"\n",
    "    Trouve une Url cellar dans un element xml URI\n",
    "    :param root: elem xmltree URI\n",
    "    :return: la valeur du champs value si l'url est de type cellar, \"\" sinon\n",
    "    \"\"\"\n",
    "    dico = {}\n",
    "    for elem in root:\n",
    "        dico[elem.tag] = elem.text\n",
    "    try:\n",
    "        if dico['{http://eur-lex.europa.eu/search}TYPE'] == 'cellar':\n",
    "            return dico['{http://eur-lex.europa.eu/search}VALUE']\n",
    "        else:\n",
    "            return \"\"\n",
    "    except Exception as e:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def get_date(root):\n",
    "    \"\"\"\n",
    "    Trouve une date dans le xml et la convertit au format date yyyy-mm-dd\n",
    "    :param root: elem xmltree contenant la date à convertir\n",
    "    :return: la date sous le bon format\n",
    "    \"\"\"\n",
    "    for elem in root:\n",
    "        if elem.tag == '{http://eur-lex.europa.eu/search}VALUE':\n",
    "            date_doc = datetime.strptime(elem.text, '%Y-%m-%d').date()\n",
    "            return date_doc\n",
    "\n",
    "\n",
    "def getinfos(root, dico, ans):\n",
    "    \"\"\"\n",
    "    Parcourt récusrivement un xml pour trouver les c et les WORK_DATE_DOCUMENT de l'xml retourné par le web service\n",
    "    :param root: elem xmltree contenant les informations du ne=oeud actif du xml\n",
    "    :param dico: dictionnaire contenant les couples WORK_DATE_DOCUMENT:URI\n",
    "    :param ans: Dernière URI trouvée (pour l'associer à la prochaine date que l'on trouve\n",
    "    :return: dict et ans\n",
    "    \"\"\"\n",
    "    if not list(root):\n",
    "        return dico, ans\n",
    "    else:\n",
    "        if root.tag == '{http://eur-lex.europa.eu/search}URI':\n",
    "            ans = get_value_if_cellar(root)\n",
    "        elif root.tag == '{http://eur-lex.europa.eu/search}WORK_DATE_DOCUMENT':\n",
    "            try:\n",
    "                dico[get_date(root)] = ans\n",
    "            except Exception as e:\n",
    "                pass\n",
    "        else:\n",
    "            for elem in root:\n",
    "                dico, ans = getinfos(elem, dico, ans)\n",
    "    return dico, ans\n",
    "\n",
    "\n",
    "def get_url_api_from_xmlstring(xmlstring):\n",
    "    \"\"\"\n",
    "    Cherche et retourne l'url cellar correspondant a la date la plus récente d'un fichier xml envoyé par le webservice\n",
    "    :param xmlstring: le xml en format str\n",
    "    :return: l'url cellar pour l'appel à l'api\n",
    "    \"\"\"\n",
    "    tree = ET.ElementTree(ET.fromstring(xmlstring))\n",
    "    root = tree.getroot()\n",
    "    dico, ans = getinfos(root, {}, \"\")\n",
    "    for elem in sorted(dico.keys(), reverse=True):\n",
    "        if dico[elem]:\n",
    "            return dico[elem]\n",
    "\n",
    "\n",
    "def check_if_error(xmlstring):\n",
    "    \"\"\"\n",
    "    Vérifie si le fichier xml envoyé par le webservice est un message d'erreur\n",
    "    :param xmlstring: le xml en format str\n",
    "    :return: \"\" s'il n'y a pas d'erreur, le message d'erreur sinon\n",
    "    \"\"\"\n",
    "    tree = ET.ElementTree(ET.fromstring(xmlstring))\n",
    "    root = tree.getroot()\n",
    "    tags = [i.tag for i in root.iter()]\n",
    "    if '{http://www.w3.org/2003/05/soap-envelope}Fault' in tags:\n",
    "        for elem in root.iter('{http://www.w3.org/2003/05/soap-envelope}Reason'):\n",
    "            text = elem.find('{http://www.w3.org/2003/05/soap-envelope}Text').text\n",
    "            return text\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def get_celex_id(url):\n",
    "    \"\"\"\n",
    "    Obtient l'id celex a partir d'une url eurlex\n",
    "    :param url: url d'où extraire le celex\n",
    "    :return: l'id celex\n",
    "    \"\"\"\n",
    "    celex_id = url.split(\"/\")[7].replace('%', ':').split(':')[1].split('&')[0].split('-')[0]\n",
    "    celex_id = celex_id.split('(')[0] + '*' if '(' in celex_id else celex_id\n",
    "    return celex_id\n",
    "\n",
    "\n",
    "def get_url_cellar(celex_id):\n",
    "    \"\"\"\n",
    "    obtient deux json pour un celex donné, un résultat en Anglais un résultat en Français\n",
    "    :param celex_id: id_celex pour l'appel au web service\n",
    "    :return: une liste contenant deux json avec l'url cellar, l'html donné par l'api et le langage de la réponse\n",
    "    \"\"\"\n",
    "    url_web_service = \"https://eur-lex.europa.eu/EURLexWebService?wsdl\"\n",
    "\n",
    "\n",
    "    # headers\n",
    "    headers = {'Content-Type': 'application/soap+xml; charset=utf-8'}\n",
    "    ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "    body = f\"\"\"\n",
    "            <soap:Envelope xmlns:sear=\"http://eur-lex.europa.eu/search\"\n",
    "            xmlns:soap=\"http://www.w3.org/2003/05/soap-envelope\">\n",
    "            <soap:Header>\n",
    "            <wsse:Security soap:mustUnderstand=\"1\" xmlns:wsse=\"http://docs.oasis-open.org/wss/2004/01/oasis-200401-wss-wssecurity-secext-1.0.xsd\">\n",
    "            <wsse:UsernameToken wsu:Id=\"UsernameToken-3\" xmlns:wsu=\"http://docs.oasis-open.org/wss/2004/01/oasis-200401-wss-wssecurity-utility-1.0.xsd\">\n",
    "            <wsse:Username>{LOGIN_EURLEX}</wsse:Username>\n",
    "            <wsse:Password Type=\"http://docs.oasis-open.org/wss/2004/01/oasis-200401-wss-username-token-profile-1.0#PasswordText\">{MOT_DE_PASSE_EURLEX}</wsse:Password>\n",
    "            </wsse:UsernameToken>\n",
    "            </wsse:Security>\n",
    "            </soap:Header>\n",
    "            <soap:Body>\n",
    "            <sear:searchRequest>\n",
    "            <sear:expertQuery>SELECT CELLAR_ID\n",
    "                                WHERE DN ~ {celex_id}\n",
    "                </sear:expertQuery>\n",
    "                <sear:page>1</sear:page>\n",
    "            <sear:pageSize>2</sear:pageSize>\n",
    "            <sear:searchLanguage>fr</sear:searchLanguage>\n",
    "            </sear:searchRequest>\n",
    "            </soap:Body>\n",
    "            </soap:Envelope>\n",
    "            \"\"\"\n",
    "\n",
    "    response = requests.post(url_web_service, data=body, headers=headers, verify=False)\n",
    "\n",
    "    erreur = check_if_error(response.text)\n",
    "    if erreur == 'Service temporarily not available':\n",
    "        time.sleep(1)\n",
    "        response = requests.post(url_web_service, data=body, headers=headers, verify=False)\n",
    "\n",
    "    return get_url_api_from_xmlstring(response.text)\n",
    "\n",
    "\n",
    "def get_api_result(url_api):\n",
    "    \"\"\"\n",
    "    Obtient deux json pour une adresse cellar donnée,un résultat en Anglais un résultat en Français\n",
    "    :param url_api: url cellar pour l'appel API\n",
    "    :return:une liste de dictionnaires conenant l'url, le contenu et la langue des résultats\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "\n",
    "    api_result = []\n",
    "    for lang in ['fra']:\n",
    "        headers_api = {'Accept': 'application/xhtml+xml;',\n",
    "                       'Accept-Language': lang}\n",
    "\n",
    "        response_api = requests.get(url_api, headers=headers_api, verify=False)\n",
    "\n",
    "        api_result.append({'url': response_api.url, 'content': response_api.text, 'lang': lang})\n",
    "    return api_result\n",
    "\n",
    "\n",
    "def get_doc_name(soup):\n",
    "    \"\"\"\n",
    "    Obtient le nom du document à partir d'un html\n",
    "    :param soup: Beautiful soup du html traité\n",
    "    :return: le nom recomposé du document\n",
    "    \"\"\"\n",
    "    text_names = soup.find_all(True, {'class': [\"title-doc-first\", \"doc-ti\", \"oj-doc-ti\"]})\n",
    "    name_list = [str(i.text) for i in text_names[:3]]\n",
    "    text_name = ' '.join(name_list)\n",
    "    text_name = text_name.replace('\\n', '').replace(u\"\\u00a0\", \" \")\n",
    "    text_name = text_name.split(re.findall('\\d+',text_name)[0],1)[1] if '\\u25ba' in text_name else text_name\n",
    "    return text_name.strip().capitalize()\n",
    "\n",
    "\n",
    "def get_content_from_html(soup):\n",
    "    \"\"\"\n",
    "    Obtient le texte d'un html\n",
    "    :param soup: Beautiful soup du html traité\n",
    "    :return: le texte\n",
    "    \"\"\"\n",
    "    for match in soup.findAll(\"span\"):\n",
    "        match.unwrap()\n",
    "    tags = soup.find_all(True, {'class': [\"norm\", \"stitle-article-norm\", \"boldface\", \"normal\", \"oj-normal\", \"note\",\n",
    "                                          \"tbl-txt\", \"tbl-hdr\", \"tbl-norm\", \"norm inline-element\", \"title-doc-first\"]})\n",
    "    text_liste = [str(i.text) for i in tags]\n",
    "    text = ' '.join(text_liste).replace(u\"\\u00a0\", \" \").replace('\\n', '')\n",
    "    return text\n",
    "\n",
    "\n",
    "def traitement_eurlex(logger):\n",
    "    \"\"\"\n",
    "    Traite tous les json contenus dans le dossier all_json de eurlex\n",
    "    :param logger: un logger\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    file_source = \"sources/Eurlex/all_json\"\n",
    "    result = (y for x in os.walk(file_source) for y in glob(os.path.join(x[0], '*.json')))\n",
    "\n",
    "    for path in result:\n",
    "        try:\n",
    "            dict_json = jso.get_json_infos(path)\n",
    "            url_fich_metier = dict_json['url']\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Problème extraction données json: {path} erreur: {e}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            celex_id = get_celex_id(url_fich_metier)\n",
    "            url_api = get_url_cellar(celex_id)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Problème web service: {url_fich_metier}\")\n",
    "            logger.info(f\"url : {url_fich_metier}, erreur : Problème pendant la requête au web service:{e}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            api_result = get_api_result(url_api)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Problème api: {url_fich_metier}\")\n",
    "            logger.info(f\"url : {url_fich_metier}, erreur : Problème pendant la requête à l'api:{e}\")\n",
    "            continue\n",
    "\n",
    "        for elem in api_result:\n",
    "            try:\n",
    "                output_file = os.path.splitext(path)[0].split('/')[-1] + '_' + elem['lang']\n",
    "                with open(f\"sources/Eurlex/data/{output_file}.html\", \"w\") as f:\n",
    "                    f.write(elem['content'])\n",
    "                soup = BeautifulSoup(elem['content'], 'html.parser')\n",
    "                text = get_content_from_html(soup)\n",
    "                dict_json['text_name'] = get_doc_name(soup)\n",
    "                dict_json = jso.clean_content(text, dict_json)\n",
    "                dict_json['url'] = elem['url']\n",
    "                dict_json['extension'] = '.html'\n",
    "                jdoc = jso.creation_jdoc(dict_json)\n",
    "                jdoc['url_fich_metier'] = url_fich_metier\n",
    "                date_collect = str(date.today())\n",
    "                output = f\"sources/Eurlex/final_json/{date_collect}_{output_file}.json\"\n",
    "                with open(output, 'w') as fp:\n",
    "                    json.dump(jdoc, fp, indent=4)\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Problème traitement réponse api: {url_fich_metier}\")\n",
    "                logger.info(f\"url : {url_fich_metier}, erreur : {e}\")\n",
    "\n",
    "url_api_AML5 ='https://eur-lex.europa.eu/legal-content/FR/TXT/HTML/?uri=CELEX:02015L0849-20240709'\n",
    "url_api_AML6 = 'https://eur-lex.europa.eu/legal-content/FR/TXT/HTML/?uri=OJ:L_202401640'\n",
    "url_api_R_AML6 = 'https://eur-lex.europa.eu/legal-content/FR/TXT/HTML/?uri=OJ:L_202401689'\n",
    "api_result_AML5 = get_api_result(url_api_AML5)\n",
    "api_result_AML6 = get_api_result(url_api_AML6)\n",
    "api_result_R_AML6 = get_api_result(url_api_R_AML6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15077/3579417624.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython.display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "for elem in api_result_AML5:\n",
    "        soup = BeautifulSoup(elem['content'], 'html.parser')\n",
    "        text = get_content_from_html(soup)\n",
    "#display(HTML(elem['content']))\n",
    "print(type(elem['content']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import AzureOpenAI\n",
    "import requests\n",
    "import openai\n",
    "import httpx\n",
    "\n",
    "# Ne pas toucher aux deux lignes suivantes.\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "# Le modèle à utiliser pour vos requêtes. Modèle recommandé : \"gpt-35-turbo\"\n",
    "# Ces modèles alternatifs ne sont à utiliser que de façon parcimonieuse.\n",
    "os.environ[\"AZURE_OPENAI_DeploymentId\"] = \"gpt-4o-mini\"\n",
    "def call_gpt(html):\n",
    "  client = AzureOpenAI(\n",
    "    azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"), \n",
    "    api_key=os.getenv(\"AZURE_OPENAI_KEY\"),  \n",
    "    api_version=\"2024-03-01-preview\",\n",
    "\n",
    "  )\n",
    "\n",
    "  exemple_output = '''\n",
    "  {\n",
    "        \"numero\": \"2\",\n",
    "        \"paragraphe\": [\n",
    "        {\n",
    "            \"numero\": \"1\",\n",
    "            \"contenu\": \"La présente directive s'applique aux entités assujetties suivantes:\",\n",
    "            \"sous_paragraphe\": [\n",
    "            {\n",
    "                \"numero\": \"1\",\n",
    "                \"contenu\": \"les personnes physiques ou morales suivantes, agissant dans l'exercice de leur activité professionnelle:\",\n",
    "                \"sous_paragraphe\": [\n",
    "                {\n",
    "                    \"lettre\": \"a\",\n",
    "                    \"contenu\": \"les notaires et autres membres de professions juridiques indépendantes, lorsqu'ils participent, au nom de leur client et pour le compte de celui-ci, à toute transaction financière ou immobilière ou lorsqu'ils assistent leur client dans la préparation ou l'exécution de transactions portant sur:\",\n",
    "                    \"sous_paragraphe\": [\n",
    "                    {\n",
    "                        \"numero\": \"i\",\n",
    "                        \"contenu\": \"l'achat et la vente de biens immeubles ou d'entreprises commerciales;\"\n",
    "                    },\n",
    "                    \n",
    "                ]\n",
    "            }\n",
    "            ]\n",
    "        }\n",
    "        ]\n",
    "    }\n",
    "    ]\n",
    "  }'''\n",
    "  response = client.chat.completions.create(\n",
    "      model=os.getenv(\"AZURE_OPENAI_DeploymentId\"), # model = \"deployment_name\".\n",
    "      response_format={ \"type\": \"json_object\" },\n",
    "      messages=[\n",
    "          {\"role\": \"system\", \"content\": f'''Tu es un extracteur de texte juridique en json, tu ne dois vraiment renvoyer que le json et seulement l'intérieur comme cette exemple: {exemple_output}\n",
    "          Tu dois inclure l'ensemble des points, sous paragraphes et sous-sous paragraphe si ils existent tu ne dois rien oublier de ce que l'on t'envoie'''},\n",
    "          {\"role\": \"user\", \"content\": f\"Peux tu me ressortir le json avec comme information le numéro de l'article, les numeros de paragraphe les sous paragraphe et le contenu de ce texte : {html}\"},\n",
    "      ],\n",
    "      temperature = 0\n",
    "  )\n",
    "  return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "art_1\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Chaîne JSON\n",
    "json_data = {\n",
    "    \"articles\": []\n",
    "}\n",
    "\n",
    "# Parser le HTML\n",
    "soup = BeautifulSoup(elem['content'], 'html.parser')\n",
    "\n",
    "# Trouver toutes les subdivisions\n",
    "articles = soup.find_all('div', class_='eli-subdivision')\n",
    "\n",
    "data = []\n",
    "\n",
    "for article in articles:\n",
    "    # Vérifier si c'est un article (ID contient 'art_')\n",
    "    article_id = article.get('id', '')\n",
    "    if not article_id.startswith('art_'):\n",
    "        continue\n",
    "    print(article_id)\n",
    "    #print(call_gpt(article.get_text()))\n",
    "    json_data[\"articles\"].append(json.loads(call_gpt(article.get_text())))\n",
    "# Nom du fichier où vous voulez sauvegarder le JSON\n",
    "fichier_json = \"articles_AML5.json\"\n",
    "\n",
    "# Écriture dans le fichier\n",
    "with open(fichier_json, \"w\", encoding=\"utf-8\") as fichier:\n",
    "    json.dump(json_data, fichier, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Parser le HTML\n",
    "soup = BeautifulSoup(elem['content'], 'html.parser')\n",
    "\n",
    "# Extraire le titre de l'article\n",
    "article_titles = soup.find_all('p', class_='title-article-norm')\n",
    "\n",
    "# Extraire les données des paragraphes\n",
    "data = []\n",
    "for article_title in article_titles:\n",
    "    paragraphs = soup.find_all('div', class_='norm', recursive=True)\n",
    "\n",
    "    for paragraph in paragraphs:\n",
    "        paragraph_number_tag = paragraph.find('span', class_='no-parag')\n",
    "        paragraph_number = paragraph_number_tag.text.strip() if paragraph_number_tag else None\n",
    "\n",
    "        # Vérifier les sous-paragraphes\n",
    "        sub_paragraphs = paragraph.find_all('div', class_='grid-container')\n",
    "        if sub_paragraphs:\n",
    "            for sub in sub_paragraphs:\n",
    "                sub_number = sub.find('span').text.strip() if sub.find('span') else None\n",
    "                sub_content = sub.find('div', class_='grid-list-column-2').text.strip() if sub.find('div', class_='grid-list-column-2') else None\n",
    "                data.append([article_title, paragraph_number, sub_number, sub_content])\n",
    "        else:\n",
    "            # Aucun sous-paragraphe trouvé\n",
    "            content = paragraph.find('div', class_='norm inline-element').text.strip() if paragraph.find('div', class_='norm inline-element') else None\n",
    "            data.append([article_title, paragraph_number, None, content])\n",
    "\n",
    "# Convertir en DataFrame pour un affichage structuré\n",
    "df = pd.DataFrame(data, columns=['Article', 'Paragraphe', 'Sous-paragraphe', 'Contenu'])\n",
    "\n",
    "# Afficher le tableau\n",
    "print(df.dropna(subset='Paragraphe'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('test.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
